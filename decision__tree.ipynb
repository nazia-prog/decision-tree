{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "ans- A Decision Tree is a powerful and intuitive supervised learning algorithm used for both classification and regression tasks. Here's how it works in the context of classification:\n",
        "\n",
        "üå≥ What Is a Decision Tree?\n",
        "A Decision Tree is a flowchart-like structure where:\n",
        "- Internal nodes represent tests on features (e.g., \"Is age > 30?\")\n",
        "- Branches represent outcomes of those tests\n",
        "- Leaf nodes represent class labels (e.g., \"Approved\", \"Denied\")\n",
        "It mimics human decision-making by breaking down a complex decision into a series of simpler decisions.\n",
        "\n",
        "‚öôÔ∏è How It Works for Classification\n",
        "- Feature Selection:\n",
        "- The algorithm chooses the best feature to split the data based on a criterion like:\n",
        "- Gini Impurity\n",
        "- Entropy (Information Gain)\n",
        "- Chi-square\n",
        "- Recursive Splitting:\n",
        "- The dataset is split into subsets based on the selected feature.\n",
        "- This process continues recursively, creating branches until:\n",
        "- All samples in a node belong to the same class\n",
        "- A stopping condition is met (e.g., max depth, min samples per leaf)\n",
        "- Prediction:\n",
        "- For a new input, the tree is traversed from root to leaf by evaluating feature conditions.\n",
        "- The leaf node reached gives the predicted class.\n",
        "\n",
        "\n",
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "ans- Gini Impurity and Entropy are two fundamental metrics used to measure the impurity or disorder of a dataset in Decision Trees. They guide the tree in selecting the best feature to split on at each node.\n",
        "\n",
        "üîç Gini Impurity\n",
        "Definition:\n",
        "Gini Impurity measures the probability of incorrectly classifying a randomly chosen element if it was randomly labeled according to the distribution of labels in the node.\n",
        "Formula:\n",
        "For a node with classes C_1, C_2, ..., C_k,\n",
        "Gini = 1 - \\sum_{i=1}^{k} p_i^2\n",
        "where p_i is the probability of class C_i.\n",
        "Interpretation:\n",
        "\n",
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision trees? Give one practical advantage of using each.\n",
        "\n",
        "ans- Pre-Pruning (Early Stopping)\n",
        "Definition: Pre-pruning halts the tree growth early‚Äîbefore it becomes overly complex. It uses criteria like maximum depth, minimum number of samples per node, or minimum information gain to decide when to stop splitting.\n",
        "Advantage:\n",
        "‚úÖ Efficiency ‚Äî It reduces training time and memory usage by preventing the tree from growing unnecessarily deep, which is especially useful for large datasets or real-time applications.\n",
        "\n",
        "‚úÇÔ∏è Post-Pruning (Reduced Error Pruning)\n",
        "Definition: Post-pruning allows the tree to grow fully and then prunes back branches that do not improve performance on a validation set. It simplifies the tree after it's built.\n",
        "Advantage:\n",
        "‚úÖ Better Generalization ‚Äî It helps reduce overfitting by removing branches that capture noise, often resulting in improved accuracy on unseen data.\n",
        "\n",
        "If you're implementing decision trees in Python (e.g., with scikit-learn), pre-pruning is typically done via parameters like max_depth, min_samples_split, etc., while post-pruning may require custom logic or libraries like cost-complexity pruning in DecisionTreeClassifier.\n",
        "\n",
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "ans- Information Gain (IG) measures how much ‚Äúinformation‚Äù a feature gives us about the target variable. In decision trees, it quantifies the reduction in entropy (or impurity) after a dataset is split on a feature.\n",
        "Mathematically:\n",
        "\\text{Information Gain} = \\text{Entropy (Parent)} - \\sum \\left( \\frac{n_i}{n} \\times \\text{Entropy (Child}_i) \\right)\n",
        "Where:\n",
        "- Entropy measures disorder or uncertainty.\n",
        "- n is the total number of samples.\n",
        "- n·µ¢ is the number of samples in child node i.\n",
        "\n",
        "üåü Why is it Important for Splitting?\n",
        "When building a decision tree, we want to split the data in a way that maximally reduces uncertainty about the target. Information Gain helps us:\n",
        "- Rank features based on how well they separate the data.\n",
        "- Choose the best split at each node to make the tree more accurate and efficient.\n",
        "\n",
        "‚úÖ Practical Impact\n",
        "- High IG ‚Üí Feature creates purer child nodes ‚Üí Better classification or regression performance.\n",
        "- Low IG ‚Üí Feature doesn‚Äôt help much ‚Üí Likely ignored during splitting.\n",
        "\n",
        "\n",
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "ans-  Real-World Applications of Decision Trees\n",
        "\n",
        "1 healthcare\n",
        "2 manufacturing\n",
        "3 education\n",
        "4 finance\n",
        "5 marketing\n",
        "6 retail\n",
        "\n",
        "‚úÖ Advantages of Decision Trees\n",
        "- Interpretability: Easy to visualize and explain to non-technical stakeholders.\n",
        "- No Need for Feature Scaling: Works well with both numerical and categorical data.\n",
        "- Handles Non-linear Relationships: Captures complex patterns without requiring transformations.\n",
        "- Fast Inference: Once trained, predictions are quick and efficient.\n",
        "\n",
        "‚ö†Ô∏è Limitations of Decision Trees\n",
        "- Overfitting: Especially with deep trees, they can memorize training data.\n",
        "- Instability: Small changes in data can lead to very different trees.\n",
        "- Bias Toward Features with More Levels: Can favor splits on categorical features with many unique values.\n",
        "- Lower Predictive Power Alone: Often outperformed by ensemble methods like Random Forests or Gradient Boosted Trees.\n",
        "\n"
      ],
      "metadata": {
        "id": "-o5-YwsBGBst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset Info:\n",
        "#Iris Dataset for classification tasks (sklearn.datasets.load_iris() or provided CSV).\n",
        "# Boston Housing Dataset for regression tasks (sklearn.datasets.load_boston() or provided CSV).\n",
        "#Question 6: Write a Python program to:\n",
        "# Load the Iris Dataset\n",
        "# Train a Decision Tree Classifier using the Gini criterion\n",
        "# Print the model‚Äôs accuracy and feature importances\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier using the Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-86Fm3dLi7b",
        "outputId": "d88051b4-bf48-4a13-ea37-72ac12441743"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Write a Python program to:\n",
        "# Load the Iris Dataset\n",
        "# Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree with max_depth=3\n",
        "tree_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_limited.fit(X_train, y_train)\n",
        "y_pred_limited = tree_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Train fully-grown Decision Tree\n",
        "tree_full = DecisionTreeClassifier(random_state=42)\n",
        "tree_full.fit(X_train, y_train)\n",
        "y_pred_full = tree_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_limited:.2f}\")\n",
        "print(f\"Accuracy with fully-grown tree: {accuracy_full:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pfZQ7h3MOqM",
        "outputId": "bf773533-8501-4529-99f9-6f025420ff9c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.00\n",
            "Accuracy with fully-grown tree: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "#Load the California Housing dataset from sklearn\n",
        "# Train a Decision Tree Regressor\n",
        "# Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "feature_names = housing.feature_names\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate MSE\n",
        "y_pred = regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(feature_names, regressor.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkvbiQQfMx3z",
        "outputId": "4d8e4d16-192f-4411-ba8b-4616e13cbd59"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.53\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5235\n",
            "HouseAge: 0.0521\n",
            "AveRooms: 0.0494\n",
            "AveBedrms: 0.0250\n",
            "Population: 0.0322\n",
            "AveOccup: 0.1390\n",
            "Latitude: 0.0900\n",
            "Longitude: 0.0888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Write a Python program to:\n",
        "# Load the Iris Dataset\n",
        "# Tune the Decision Tree‚Äôs max_depth and min_samples_split using GridSearchCV\n",
        "# Print the best parameters and the resulting model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "# Initialize Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Perform Grid Search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters and evaluate on test set\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Output results\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Model Accuracy on Test Set: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4rLZYRSNTDB",
        "outputId": "c67ec38f-8697-45bc-d533-e5bcbc00b524"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
            "Model Accuracy on Test Set: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you‚Äôre working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "‚óè Handle the missing values\n",
        "‚óè Encode the categorical features\n",
        "‚óè Train a Decision Tree model\n",
        "‚óè Tune its hyperparameters\n",
        "‚óè Evaluate its performance And describe what business value this model could provide in the real-world setting\n",
        "\n",
        "ans-  Step-by-Step Workflow for Disease Prediction\n",
        "1Ô∏è‚É£ Handle Missing Values\n",
        "- Numerical Features:\n",
        "- Use mean or median imputation (SimpleImputer from sklearn) depending on distribution.\n",
        "- For time-sensitive or correlated data, consider regression imputation or KNN imputation.\n",
        "- Categorical Features:\n",
        "- Use mode imputation or fill with \"Unknown\" if missingness is informative.\n",
        "- Optionally, add a missingness indicator column to capture patterns.\n",
        "2Ô∏è‚É£ Encode Categorical Features\n",
        "- Low-cardinality features:\n",
        "- Use One-Hot Encoding (pd.get_dummies() or OneHotEncoder) to avoid ordinal assumptions.\n",
        "- High-cardinality features:\n",
        "- Use Target Encoding or Frequency Encoding to reduce dimensionality.\n",
        "- Tree-based models like Decision Trees can handle label encoding (LabelEncoder) without loss of performance.\n",
        "3Ô∏è‚É£ Train a Decision Tree Model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "- Use train_test_split() to separate training and testing data.\n",
        "- Normalize only if using models sensitive to scale (not needed for trees).\n",
        "4Ô∏è‚É£ Tune Hyperparameters\n",
        "Use GridSearchCV or RandomizedSearchCV to optimize:\n",
        "- max_depth: Controls tree complexity.\n",
        "- min_samples_split: Minimum samples to split a node.\n",
        "- min_samples_leaf: Minimum samples at a leaf node.\n",
        "- max_features: Number of features to consider at each split.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 10, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "grid = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "5Ô∏è‚É£ Evaluate Performance\n",
        "- Accuracy: Overall correctness.\n",
        "- Precision & Recall: Especially important in healthcare (e.g., false negatives).\n",
        "- F1 Score: Balances precision and recall.\n",
        "- ROC-AUC: Measures model‚Äôs ability to distinguish between classes.\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "y_pred = grid.best_estimator_.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, grid.best_estimator_.predict_proba(X_test)[:,1]))\n",
        "\n",
        "Ó∑ôÓ∑ö\n",
        "\n",
        "üíº Business Value in Healthcare\n",
        "- Early Detection: Helps identify high-risk patients before symptoms escalate.\n",
        "- Resource Optimization: Prioritizes diagnostic testing for likely cases, reducing costs.\n",
        "- Personalized Care: Enables tailored treatment plans based on predicted risk.\n",
        "- Compliance & Auditability: Decision Trees are interpretable, aiding regulatory compliance.\n",
        "- Patient Engagement: Transparent models build trust with patients and clinicians.\n",
        "\n"
      ],
      "metadata": {
        "id": "b4fBahiiOlIl"
      }
    }
  ]
}